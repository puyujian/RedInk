"""RQ Worker å¯åŠ¨è„šæœ¬ï¼ˆæ”¯æŒå¤šè¿›ç¨‹å¹¶å‘ï¼‰ã€‚

æ”¯æŒå¤š Worker è¿›ç¨‹å¹¶è¡Œå¤„ç†ä»»åŠ¡ï¼Œè§£å†³å¤šç”¨æˆ·åŒæ—¶å‘å¸ƒä»»åŠ¡æ—¶çš„æ’é˜Ÿé—®é¢˜ã€‚
é€šè¿‡ WORKER_CONCURRENCY ç¯å¢ƒå˜é‡æ§åˆ¶å¹¶å‘æ•°é‡ï¼ˆé»˜è®¤ 4ï¼‰ã€‚

ä½¿ç”¨æ–¹å¼:
    python backend/worker.py

ç¯å¢ƒå˜é‡:
    WORKER_CONCURRENCY: å¹¶å‘ Worker æ•°é‡ï¼ˆé»˜è®¤ 4ï¼Œä¸Šé™ä¸º CPU æ ¸å¿ƒæ•° * 2ï¼‰
    REDIS_URL: Redis è¿æ¥åœ°å€
    WORKER_LOG_LEVEL: æ—¥å¿—çº§åˆ«ï¼ˆé»˜è®¤ INFOï¼Œå¯é€‰ DEBUG/WARNING/ERRORï¼‰
    WORKER_LOG_FILE: æ—¥å¿—æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œä¸è®¾ç½®åˆ™ä»…è¾“å‡ºåˆ°æ§åˆ¶å°ï¼‰
"""

from __future__ import annotations

import atexit
import contextlib
import logging
import multiprocessing
import os
import platform
import signal
import sys
import time
from multiprocessing.connection import wait as mp_wait
from pathlib import Path
from typing import List, Optional

# ç¡®ä¿é¡¹ç›®æ ¹ç›®å½•åœ¨ sys.path ä¸­
project_root = Path(__file__).parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

from backend.config import Config

# ä¸»è¿›ç¨‹æ—¥å¿—é…ç½®ï¼ˆæ”¯æŒç¯å¢ƒå˜é‡è°ƒæ•´çº§åˆ«ä¸æ–‡ä»¶è¾“å‡ºï¼‰
_LOG_LEVEL = os.getenv("WORKER_LOG_LEVEL", "INFO").upper()
_LOG_FILE = os.getenv("WORKER_LOG_FILE")
_LOG_HANDLERS: List[logging.Handler] = [logging.StreamHandler()]
if _LOG_FILE:
    _LOG_HANDLERS.append(logging.FileHandler(_LOG_FILE, encoding="utf-8"))

logging.basicConfig(
    level=getattr(logging, _LOG_LEVEL, logging.INFO),
    format="%(asctime)s [%(levelname)s] %(name)s: %(message)s",
    datefmt="%H:%M:%S",
    handlers=_LOG_HANDLERS,
)
logger = logging.getLogger(__name__)

# å…¨å±€åœæ­¢æ ‡å¿—
_shutdown_requested = False

# è¿›ç¨‹åˆ—è¡¨ï¼ˆæ¨¡å—çº§å˜é‡ï¼Œä¾¿äºå¼‚å¸¸å¤„ç†æ—¶è®¿é—®ï¼‰
_worker_processes: List[multiprocessing.Process] = []

# é‡å¯èŠ‚æµé…ç½®ï¼Œé¿å…å¼‚å¸¸é‡å¯é£æš´
_RESTART_BACKOFF_SECONDS = 5

# RQ Worker å¿ƒè·³ TTLï¼ˆç§’ï¼‰ï¼Œç”¨äºåˆ¤æ–­ worker æ˜¯å¦å¤±æ•ˆ
_DEFAULT_WORKER_TTL = int(os.getenv("RQ_WORKER_TTL", "420"))


def _pid_alive(pid: int) -> bool:
    """æ£€æŸ¥æœ¬æœº PID æ˜¯å¦ä»ç„¶å­˜æ´»ã€‚

    Args:
        pid: è¿›ç¨‹ ID

    Returns:
        True å¦‚æœè¿›ç¨‹å­˜æ´»ï¼ŒFalse å¦‚æœè¿›ç¨‹ä¸å­˜åœ¨æˆ–å·²é€€å‡º
    """
    if pid <= 0:
        return False
    try:
        # os.kill(pid, 0) ä¸ä¼šçœŸæ­£å‘é€ä¿¡å·ï¼Œåªæ£€æŸ¥è¿›ç¨‹æ˜¯å¦å­˜åœ¨
        os.kill(pid, 0)
    except OSError:
        return False
    return True


def _cleanup_stale_worker(
    redis_conn,
    worker_name: str,
    worker_ttl: int = _DEFAULT_WORKER_TTL,
    process_logger: Optional[logging.Logger] = None,
) -> bool:
    """æ¸…ç† Redis ä¸­æ®‹ç•™çš„åŒååƒµå°¸ worker æ³¨å†Œä¿¡æ¯ã€‚

    åœ¨ worker.register_birth() å‰è°ƒç”¨ï¼Œé¿å…å› æ®‹ç•™æ³¨å†Œå¯¼è‡´å¯åŠ¨å¤±è´¥ã€‚
    ä½¿ç”¨ Redis åˆ†å¸ƒå¼é”é˜²æ­¢å¹¶å‘æ¸…ç†å†²çªã€‚

    æ¸…ç†æ¡ä»¶ï¼ˆæ»¡è¶³ä»»ä¸€ï¼‰ï¼š
    1. å¿ƒè·³ key å·²ä¸å­˜åœ¨ï¼ˆTTL è¿‡æœŸè‡ªåŠ¨åˆ é™¤ï¼‰ï¼Œä½†é›†åˆä¸­ä»æœ‰æ®‹ç•™
    2. å¿ƒè·³ key å­˜åœ¨ä½†å¯¹åº” PID å·²ä¸å­˜æ´»ï¼ˆè¿›ç¨‹å¼‚å¸¸é€€å‡ºï¼‰
    3. å¿ƒè·³æ—¶é—´æˆ³è¶…è¿‡ worker_ttlï¼ˆå¿ƒè·³é•¿æ—¶é—´æœªæ›´æ–°ï¼‰

    Args:
        redis_conn: Redis è¿æ¥å¯¹è±¡
        worker_name: Worker åç§°
        worker_ttl: Worker å¿ƒè·³è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        process_logger: æ—¥å¿—è®°å½•å™¨ï¼ˆå¯é€‰ï¼‰

    Returns:
        True å¦‚æœæ‰§è¡Œäº†æ¸…ç†ï¼ŒFalse å¦‚æœæ— éœ€æ¸…ç†æˆ–æœªè·å–é”
    """
    log = process_logger or logger
    lock_key = f"lock:rq-worker-clean:{worker_name}"
    lock = redis_conn.lock(lock_key, timeout=10, blocking_timeout=0)

    if not lock.acquire(blocking=False):
        log.debug(f"æœªè·å–æ¸…ç†é”ï¼Œè·³è¿‡æ¸…ç†: {worker_name}")
        return False

    try:
        worker_key = f"rq:worker:{worker_name}"
        workers_set = "rq:workers"

        # æ£€æŸ¥ worker æ˜¯å¦åœ¨é›†åˆä¸­æ³¨å†Œ
        if not redis_conn.sismember(workers_set, worker_name):
            return False  # æœªæ³¨å†Œï¼Œæ— éœ€æ¸…ç†

        # æƒ…å†µ1ï¼šå¿ƒè·³ key å·²ä¸å­˜åœ¨ï¼ˆTTL=-2 è¡¨ç¤º key ä¸å­˜åœ¨ï¼‰
        ttl = redis_conn.ttl(worker_key)
        if ttl == -2:
            redis_conn.srem(workers_set, worker_name)
            log.info(f"ğŸ§¹ å·²æ¸…ç†æ®‹ç•™ workerï¼ˆå¿ƒè·³ key å·²è¿‡æœŸï¼‰: {worker_name}")
            return True

        # è·å– worker çš„ PID å’Œå¿ƒè·³ä¿¡æ¯
        worker_data = redis_conn.hgetall(worker_key)
        if not worker_data:
            redis_conn.srem(workers_set, worker_name)
            log.info(f"ğŸ§¹ å·²æ¸…ç†æ®‹ç•™ workerï¼ˆæ— å¿ƒè·³æ•°æ®ï¼‰: {worker_name}")
            return True

        # æƒ…å†µ2ï¼šPID å·²ä¸å­˜æ´»ï¼ˆæœ€å¯é çš„åˆ¤æ–­ï¼‰
        pid_str = worker_data.get(b"pid") or worker_data.get("pid")
        if pid_str:
            try:
                pid = int(pid_str)
                if not _pid_alive(pid):
                    redis_conn.delete(worker_key)
                    redis_conn.srem(workers_set, worker_name)
                    log.info(f"ğŸ§¹ å·²æ¸…ç†æ®‹ç•™ workerï¼ˆPID {pid} å·²é€€å‡ºï¼‰: {worker_name}")
                    return True
            except (ValueError, TypeError):
                pass  # PID è§£æå¤±è´¥ï¼Œç»§ç»­æ£€æŸ¥å¿ƒè·³

        # æƒ…å†µ3ï¼šå¿ƒè·³è¶…æ—¶ï¼ˆå…œåº•æ£€æŸ¥ï¼‰
        # RQ å¿ƒè·³å¯èƒ½æ˜¯ float æ—¶é—´æˆ³æˆ– UTC å­—ç¬¦ä¸²æ ¼å¼
        last_heartbeat = worker_data.get(b"last_heartbeat") or worker_data.get("last_heartbeat")
        if last_heartbeat:
            heartbeat_time: Optional[float] = None
            try:
                # å°è¯•è§£æä¸º float æ—¶é—´æˆ³
                if isinstance(last_heartbeat, bytes):
                    last_heartbeat = last_heartbeat.decode("utf-8")
                heartbeat_time = float(last_heartbeat)
            except (ValueError, TypeError):
                # å°è¯•è§£æ UTC å­—ç¬¦ä¸²æ ¼å¼ï¼ˆå¦‚ "2024-01-01 12:00:00.123456"ï¼‰
                try:
                    from datetime import datetime
                    dt = datetime.fromisoformat(str(last_heartbeat).replace(" ", "T"))
                    heartbeat_time = dt.timestamp()
                except Exception:
                    pass  # è§£æå¤±è´¥ï¼Œè·³è¿‡å¿ƒè·³æ£€æŸ¥

            if heartbeat_time is not None and time.time() - heartbeat_time > worker_ttl:
                redis_conn.delete(worker_key)
                redis_conn.srem(workers_set, worker_name)
                log.info(f"ğŸ§¹ å·²æ¸…ç†æ®‹ç•™ workerï¼ˆå¿ƒè·³è¶…æ—¶ï¼‰: {worker_name}")
                return True

        # æƒ…å†µ4ï¼šTTL ä¸ºæ°¸ä¹…ï¼ˆ-1ï¼‰ä½†æ— æ³•ç¡®å®šå­˜æ´»çŠ¶æ€ï¼Œä¿å®ˆä¸æ¸…ç†
        # è¿™ç§æƒ…å†µå¾ˆå°‘è§ï¼Œé€šå¸¸ PID æ£€æŸ¥å·²ç»è¦†ç›–

        return False

    finally:
        with contextlib.suppress(Exception):
            lock.release()


def _force_cleanup_worker(
    redis_conn,
    worker_name: str,
    process_logger: Optional[logging.Logger] = None,
) -> bool:
    """å¼ºåˆ¶æ¸…ç† worker æ³¨å†Œï¼ˆä¸æ£€æŸ¥å­˜æ´»çŠ¶æ€ï¼‰ã€‚

    ç”¨äºå¯åŠ¨æ—¶æ¸…ç†å½“å‰ä¸»æœºçš„æ‰€æœ‰æ—§æ³¨å†Œã€‚å¦‚æœå¯¹åº” PID ä»å­˜æ´»ï¼Œ
    ä¼šå…ˆå°è¯•ç»ˆæ­¢è¯¥è¿›ç¨‹ã€‚

    Args:
        redis_conn: Redis è¿æ¥å¯¹è±¡
        worker_name: Worker åç§°
        process_logger: æ—¥å¿—è®°å½•å™¨ï¼ˆå¯é€‰ï¼‰

    Returns:
        True å¦‚æœæ‰§è¡Œäº†æ¸…ç†
    """
    log = process_logger or logger
    worker_key = f"rq:worker:{worker_name}"
    workers_set = "rq:workers"

    # æ£€æŸ¥æ˜¯å¦æ³¨å†Œ
    if not redis_conn.sismember(workers_set, worker_name):
        # ä¹Ÿæ£€æŸ¥å­¤ç«‹çš„ worker key
        if not redis_conn.exists(worker_key):
            return False

    # å°è¯•è·å–å¹¶ç»ˆæ­¢æ®‹ç•™è¿›ç¨‹
    worker_data = redis_conn.hgetall(worker_key)
    if worker_data:
        pid_str = worker_data.get(b"pid") or worker_data.get("pid")
        if pid_str:
            try:
                pid = int(pid_str)
                if _pid_alive(pid):
                    log.info(f"ğŸ”ª æ­£åœ¨ç»ˆæ­¢æ®‹ç•™è¿›ç¨‹ PID {pid}: {worker_name}")
                    try:
                        os.kill(pid, signal.SIGTERM)
                        # ç»™è¿›ç¨‹ä¸€ç‚¹æ—¶é—´ä¼˜é›…é€€å‡º
                        time.sleep(0.1)
                        if _pid_alive(pid):
                            os.kill(pid, signal.SIGKILL)
                    except OSError:
                        pass  # è¿›ç¨‹å¯èƒ½å·²é€€å‡º
            except (ValueError, TypeError):
                pass

    # å¼ºåˆ¶åˆ é™¤ Redis æ³¨å†Œ
    redis_conn.delete(worker_key)
    redis_conn.srem(workers_set, worker_name)
    log.info(f"ğŸ§¹ å·²å¼ºåˆ¶æ¸…ç† worker æ³¨å†Œ: {worker_name}")
    return True


def _cleanup_all_stale_workers_for_host(redis_conn, force: bool = False) -> int:
    """æ¸…ç†å½“å‰ä¸»æœºæ‰€æœ‰æ®‹ç•™çš„åƒµå°¸ workerã€‚

    åœ¨ä¸»è¿›ç¨‹å¯åŠ¨æ—¶è°ƒç”¨ï¼Œæ‰¹é‡æ¸…ç†å½“å‰ä¸»æœºåå‰ç¼€çš„æ‰€æœ‰å†å² workerã€‚

    Args:
        redis_conn: Redis è¿æ¥å¯¹è±¡
        force: æ˜¯å¦å¼ºåˆ¶æ¸…ç†ï¼ˆTrue=ä¸æ£€æŸ¥å­˜æ´»çŠ¶æ€ç›´æ¥æ¸…ç†å¹¶ç»ˆæ­¢è¿›ç¨‹ï¼‰

    Returns:
        æ¸…ç†çš„ worker æ•°é‡
    """
    host_prefix = f"worker-{platform.node()}-"
    workers_set = "rq:workers"
    cleaned_count = 0

    try:
        # è·å–æ‰€æœ‰æ³¨å†Œçš„ worker
        all_workers = redis_conn.smembers(workers_set)
        for worker_name_bytes in all_workers:
            worker_name = (
                worker_name_bytes.decode("utf-8")
                if isinstance(worker_name_bytes, bytes)
                else worker_name_bytes
            )

            # åªæ¸…ç†å½“å‰ä¸»æœºçš„ worker
            if worker_name.startswith(host_prefix):
                if force:
                    if _force_cleanup_worker(redis_conn, worker_name):
                        cleaned_count += 1
                else:
                    if _cleanup_stale_worker(redis_conn, worker_name):
                        cleaned_count += 1

    except Exception as e:
        logger.warning(f"âš  æ‰¹é‡æ¸…ç†æ®‹ç•™ worker æ—¶å‡ºé”™: {e}")

    return cleaned_count


def _setup_child_logging(worker_id: int) -> logging.Logger:
    """ä¸ºå­è¿›ç¨‹é…ç½®ç‹¬ç«‹çš„æ—¥å¿—ã€‚

    Windows spawn æ¨¡å¼ä¸‹å­è¿›ç¨‹ä¸ä¼šç»§æ‰¿çˆ¶è¿›ç¨‹çš„ logging é…ç½®ï¼Œ
    éœ€è¦åœ¨å­è¿›ç¨‹ä¸­é‡æ–°é…ç½®ã€‚ç»§æ‰¿çˆ¶è¿›ç¨‹çš„æ—¥å¿—çº§åˆ«å’Œæ–‡ä»¶è¾“å‡ºè®¾ç½®ã€‚

    Args:
        worker_id: Worker ç¼–å·

    Returns:
        é…ç½®å¥½çš„ Logger å®ä¾‹
    """
    # ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®ï¼ˆä¸ä¸»è¿›ç¨‹ä¿æŒä¸€è‡´ï¼‰
    log_level = os.getenv("WORKER_LOG_LEVEL", "INFO").upper()
    log_file = os.getenv("WORKER_LOG_FILE")

    handlers: List[logging.Handler] = [logging.StreamHandler()]
    if log_file:
        handlers.append(logging.FileHandler(log_file, encoding="utf-8"))

    # é‡æ–°é…ç½®æ—¥å¿—ï¼ˆå­è¿›ç¨‹ç‹¬ç«‹é…ç½®ï¼‰
    logging.basicConfig(
        level=getattr(logging, log_level, logging.INFO),
        format="%(asctime)s [%(levelname)s] %(name)s: %(message)s",
        datefmt="%H:%M:%S",
        handlers=handlers,
        force=True,  # Python 3.8+ å…è®¸é‡æ–°é…ç½®
    )
    return logging.getLogger(f"worker-{worker_id}")


def _run_single_worker(worker_id: int) -> None:
    """è¿è¡Œå•ä¸ª Worker è¿›ç¨‹ã€‚

    Args:
        worker_id: Worker ç¼–å·ï¼ˆç”¨äºæ—¥å¿—åŒºåˆ†ï¼‰
    """
    # å­è¿›ç¨‹é…ç½®æ—¥å¿—
    process_logger = _setup_child_logging(worker_id)

    # å­è¿›ç¨‹éœ€è¦é‡æ–°å¯¼å…¥å’Œåˆå§‹åŒ–è¿æ¥
    from backend.task_queue import (
        get_outline_queue,
        get_image_queue,
        get_redis_connection,
        reset_connections,
    )

    # é‡ç½®è¿æ¥ç¼“å­˜ï¼Œç¡®ä¿å­è¿›ç¨‹å»ºç«‹ç‹¬ç«‹è¿æ¥
    reset_connections()

    worker_name = f"worker-{platform.node()}-{worker_id}"
    process_logger.info(f"ğŸ”§ Worker {worker_id} æ­£åœ¨åˆå§‹åŒ–...")

    try:
        # æ¯ä¸ªå­è¿›ç¨‹å»ºç«‹ç‹¬ç«‹çš„ Redis è¿æ¥
        redis_conn = get_redis_connection()
        redis_conn.ping()

        outline_queue = get_outline_queue()
        image_queue = get_image_queue()

        # é€‰æ‹© Worker ç±»å‹
        if platform.system() == "Windows":
            from rq.worker import SimpleWorker as WorkerClass
        else:
            from rq import Worker as WorkerClass

        # å­è¿›ç¨‹çº§åˆ«å¼ºåˆ¶æ¸…ç†ï¼šç¡®ä¿åŒå worker æ³¨å†Œä¸å­˜åœ¨ï¼ˆåŒé‡ä¿æŠ¤ï¼‰
        _force_cleanup_worker(
            redis_conn,
            worker_name,
            process_logger=process_logger,
        )

        worker = WorkerClass(
            queues=[outline_queue, image_queue],
            connection=redis_conn,
            name=worker_name,
        )

        process_logger.info(f"âœ“ Worker {worker_id} å·²å¯åŠ¨ï¼Œç›‘å¬é˜Ÿåˆ—ä¸­...")
        worker.work(with_scheduler=False)

    except Exception as e:
        process_logger.error(f"âŒ Worker {worker_id} å¼‚å¸¸é€€å‡º: {e}", exc_info=True)
        sys.exit(1)
    finally:
        # ç¡®ä¿è¿æ¥ä¸å¥æŸ„æ¸…ç†ï¼Œé¿å…èµ„æºæ³„æ¼
        try:
            reset_connections()
        except Exception:
            process_logger.debug("èµ„æºæ¸…ç†å¤±è´¥ï¼Œç»§ç»­é€€å‡º", exc_info=True)


def _signal_handler(signum: int, frame) -> None:
    """å¤„ç†ä¸­æ–­ä¿¡å·ï¼Œè®¾ç½®åœæ­¢æ ‡å¿—ã€‚"""
    global _shutdown_requested
    _shutdown_requested = True
    logger.info(f"\nâ¹ æ”¶åˆ°åœæ­¢ä¿¡å· ({signal.Signals(signum).name})ï¼Œæ­£åœ¨ä¼˜é›…å…³é—­...")


def _create_worker_process(worker_id: int) -> multiprocessing.Process:
    """åˆ›å»º Worker è¿›ç¨‹ã€‚

    Args:
        worker_id: Worker ç¼–å·

    Returns:
        åˆ›å»ºçš„è¿›ç¨‹å¯¹è±¡
    """
    return multiprocessing.Process(
        target=_run_single_worker,
        args=(worker_id,),
        name=f"worker-{worker_id}",
        daemon=False,  # éå®ˆæŠ¤è¿›ç¨‹ï¼Œç¡®ä¿ä¼˜é›…å…³é—­
    )


def _terminate_all_workers(processes: List[multiprocessing.Process]) -> None:
    """ç»ˆæ­¢æ‰€æœ‰ Worker è¿›ç¨‹ã€‚

    Args:
        processes: è¿›ç¨‹åˆ—è¡¨
    """
    if not processes:
        return

    logger.info("æ­£åœ¨ç»ˆæ­¢æ‰€æœ‰ Worker è¿›ç¨‹...")

    # å‘é€ç»ˆæ­¢ä¿¡å·
    for i, p in enumerate(processes):
        if p.is_alive():
            p.terminate()
            logger.info(f"âœ“ å·²å‘é€ç»ˆæ­¢ä¿¡å·ç»™ Worker {i} (PID: {p.pid})")

    # ç­‰å¾…æ‰€æœ‰è¿›ç¨‹é€€å‡º
    for p in processes:
        p.join(timeout=10)
        if p.is_alive():
            logger.warning(f"âš  Worker (PID: {p.pid}) æœªå“åº”ï¼Œå¼ºåˆ¶ç»ˆæ­¢...")
            p.kill()
            p.join(timeout=5)

    logger.info("âœ“ æ‰€æœ‰ Worker å·²åœæ­¢")


def _resolve_concurrency() -> int:
    """è§£æå¹¶å‘æ•°ï¼Œé˜²å¾¡æ€§æ ¡æ­£é…ç½®ï¼Œé¿å… CPU è¿‡è½½æˆ–é…ç½®é”™è¯¯ã€‚

    Returns:
        æ ¡æ­£åçš„å¹¶å‘ Worker æ•°é‡
    """
    try:
        cpu_total = multiprocessing.cpu_count() or 1
    except NotImplementedError:
        cpu_total = 1

    configured = Config.WORKER_CONCURRENCY or 4
    if configured <= 0:
        logger.warning("âš  WORKER_CONCURRENCY é…ç½®æ— æ•ˆï¼ˆ<=0ï¼‰ï¼Œå›é€€ä¸º 1")
        return 1

    # å…è®¸ç•¥é«˜äº CPU æ ¸å¿ƒæ•°ï¼Œå…¼é¡¾ I/O å¯†é›†å‹ä»»åŠ¡
    upper_bound = max(1, cpu_total * 2)
    if configured > upper_bound:
        logger.warning(
            f"âš  WORKER_CONCURRENCY={configured} è¿‡é«˜ï¼Œä¾æ® CPU æ ¸å¿ƒæ•°é™åˆ¶ä¸º {upper_bound}"
        )
        return upper_bound
    return configured


def _register_exit_cleanup() -> None:
    """æ³¨å†Œè¿›ç¨‹é€€å‡ºæ—¶çš„æ¸…ç†ï¼Œç¡®ä¿å¼‚å¸¸ç»ˆæ­¢ä¹Ÿèƒ½å›æ”¶å­è¿›ç¨‹ã€‚"""
    atexit.register(_terminate_all_workers, _worker_processes)


def main() -> None:
    """å¯åŠ¨å¤šè¿›ç¨‹ Worker Poolã€‚"""
    global _shutdown_requested, _worker_processes

    # è®¾ç½®ä¿¡å·å¤„ç†å™¨
    signal.signal(signal.SIGINT, _signal_handler)
    signal.signal(signal.SIGTERM, _signal_handler)
    if hasattr(signal, "SIGBREAK"):
        signal.signal(signal.SIGBREAK, _signal_handler)

    # Windows å¤šè¿›ç¨‹å¿…é¡»ä½¿ç”¨ spawn
    if platform.system() == "Windows":
        multiprocessing.set_start_method("spawn", force=True)

    try:
        # æµ‹è¯• Redis è¿æ¥
        from backend.task_queue import get_redis_connection
        redis_conn = get_redis_connection()
        redis_conn.ping()
        logger.info("âœ“ Redis è¿æ¥æˆåŠŸ")

        # å¼ºåˆ¶æ¸…ç†å½“å‰ä¸»æœºæ®‹ç•™çš„ worker æ³¨å†Œï¼ˆç»ˆæ­¢æ®‹ç•™è¿›ç¨‹å¹¶åˆ é™¤æ³¨å†Œï¼‰
        cleaned_count = _cleanup_all_stale_workers_for_host(redis_conn, force=True)
        if cleaned_count > 0:
            logger.info(f"âœ“ å·²æ¸…ç† {cleaned_count} ä¸ªæ®‹ç•™ worker æ³¨å†Œ")

        # è·å–å¹¶å‘é…ç½®ï¼ˆå¸¦é˜²å¾¡æ€§æ ¡éªŒï¼‰
        concurrency = _resolve_concurrency()
        logger.info(f"âœ“ å¹¶å‘ Worker æ•°é‡: {concurrency}")

        logger.info("=" * 60)
        logger.info("ğŸš€ RQ Worker Pool å¯åŠ¨ä¸­...")
        logger.info(f"   Worker æ•°é‡: {concurrency}")
        logger.info(f"   å¹³å°: {platform.system()}")
        logger.info("   åœæ­¢æ–¹å¼: Ctrl+C")
        logger.info("=" * 60)

        # æ³¨å†Œå¼‚å¸¸é€€å‡ºæ¸…ç†
        _register_exit_cleanup()

        # é‡å¯æ—¶é—´æˆ³è¿½è¸ªï¼Œç”¨äºèŠ‚æµ
        last_restart_ts: List[float] = [0.0 for _ in range(concurrency)]

        # å¯åŠ¨å¤šä¸ª Worker è¿›ç¨‹
        for i in range(concurrency):
            p = _create_worker_process(i)
            p.start()
            _worker_processes.append(p)
            logger.info(f"âœ“ Worker {i} è¿›ç¨‹å·²å¯åŠ¨ (PID: {p.pid})")

        # æ„å»º sentinel -> worker index æ˜ å°„ï¼Œç”¨äºé«˜æ•ˆç›‘å¬
        sentinel_index_map = {p.sentinel: i for i, p in enumerate(_worker_processes)}

        # è®°å½•èŠ‚æµæœŸé—´æ˜¯å¦å·²è­¦å‘Šï¼Œé¿å…é‡å¤æ—¥å¿—åˆ·å±
        throttle_warned: List[bool] = [False for _ in range(concurrency)]

        # ä¸»è¿›ç¨‹ç­‰å¾…å­è¿›ç¨‹ï¼Œä½¿ç”¨ sentinel ç›‘å¬æ›¿ä»£è½®è¯¢
        while not _shutdown_requested:
            # ä½¿ç”¨ mp_wait ç›‘å¬è¿›ç¨‹é€€å‡ºäº‹ä»¶ï¼Œæ¯” sleep è½®è¯¢æ›´é«˜æ•ˆ
            ready = mp_wait(list(sentinel_index_map.keys()), timeout=1.0)

            for sentinel in ready:
                i = sentinel_index_map.get(sentinel)
                if i is None:
                    continue

                p = _worker_processes[i]
                if not p.is_alive() and p.exitcode is not None:
                    # é‡å¯èŠ‚æµï¼šé¿å…çŸ­æ—¶é—´å†…å¤šæ¬¡é‡å¯
                    now = time.time()
                    time_since_last = now - last_restart_ts[i]
                    if time_since_last < _RESTART_BACKOFF_SECONDS:
                        # ä»…é¦–æ¬¡è­¦å‘Šï¼Œé¿å…æ—¥å¿—åˆ·å±
                        if not throttle_warned[i]:
                            remaining = _RESTART_BACKOFF_SECONDS - time_since_last
                            logger.warning(
                                f"âš  Worker {i} çŸ­æ—¶é—´å†…å¤šæ¬¡å´©æºƒï¼Œ{remaining:.1f}s åé‡å¯"
                            )
                            throttle_warned[i] = True
                        continue

                    # é‡ç½®èŠ‚æµè­¦å‘Šæ ‡å¿—
                    throttle_warned[i] = False
                    last_restart_ts[i] = now

                    # è®°å½•é€€å‡ºçŠ¶æ€
                    if p.exitcode != 0:
                        logger.warning(
                            f"âš  Worker {i} å¼‚å¸¸é€€å‡º (exit code: {p.exitcode})ï¼Œæ­£åœ¨é‡å¯..."
                        )
                    else:
                        logger.info(f"â„¹ Worker {i} æ­£å¸¸é€€å‡ºï¼Œæ­£åœ¨é‡å¯...")

                    # é‡å¯é€€å‡ºçš„ Worker
                    new_p = _create_worker_process(i)
                    new_p.start()
                    _worker_processes[i] = new_p

                    # æ›´æ–° sentinel æ˜ å°„
                    sentinel_index_map.pop(sentinel, None)
                    sentinel_index_map[new_p.sentinel] = i

                    logger.info(f"âœ“ Worker {i} å·²é‡å¯ (PID: {new_p.pid})")

        # æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œç»ˆæ­¢æ‰€æœ‰å­è¿›ç¨‹
        _terminate_all_workers(_worker_processes)

    except KeyboardInterrupt:
        logger.info("\nâ¹ Worker Pool å·²åœæ­¢ (KeyboardInterrupt)")
        _terminate_all_workers(_worker_processes)
        sys.exit(0)
    except Exception as e:
        logger.error(f"âŒ Worker Pool å¯åŠ¨å¤±è´¥: {e}", exc_info=True)
        _terminate_all_workers(_worker_processes)
        sys.exit(1)


if __name__ == "__main__":
    # Windows spawn æ¨¡å¼ä¸‹éœ€è¦ freeze_support ä»¥é¿å…å­è¿›ç¨‹é‡å¤æ‰§è¡Œå…¥å£
    multiprocessing.freeze_support()
    main()
